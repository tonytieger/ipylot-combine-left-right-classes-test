{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrsaDfdVHzxt"
   },
   "source": [
    "# Combining left and right hand texting/phone use classes versus leaving seperate test\n",
    "\n",
    "In this notebook, we will train and compare on a large validation set, the same training set, but with two versions, to compare the results. This notebook was ran on a PC with Anaconda and Pytorch with NVidia GPU acceleration to speed up training.\n",
    "\n",
    "We will do the following:\n",
    "\n",
    "* We have already gathered a subset of the larger dataset of labeled images for distracted driving. This is taken from the [State Farm kaggle](https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/data) competition, along with our own images, and three new classes for drowsiness were added. The four left/right texting/phone use classes were combined into two.\n",
    "* Remove and leave about 1000 training samples (100 or so from each class) and keep the 1000 or so validation set as-is.\n",
    "* Remove the \"drowsiness\" classes and images, leaving only the 8 classes from the State Farm test (with left/right combined).\n",
    "* Export our dataset to YOLOv5. This is DATA SET 1.\n",
    "* Re-seperate the left/right texting/phone use classes, so now we have 10 classes. Export our dataset to YOLOv5. This is DATA SET 2.\n",
    "* Train YOLOv5 to recognize the objects in the datasets.\n",
    "* Evaluate our YOLOv5 model's performances, and compare how the models do on each dataset.\n",
    "\n",
    "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNveqeA1KXGy"
   },
   "source": [
    "# 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTvDNSILZoN9",
    "outputId": "d8447690-10a0-4d5d-fb8d-812fb63fbb6e"
   },
   "outputs": [],
   "source": [
    "#These only need to be done once. We suggest running this notebook in a new anaconda environment.\n",
    "#Steps below are commented out since they only need to be done once. You may need to restart environment after installing.\n",
    "\n",
    "#!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "#%cd yolov5\n",
    "#%pip install -qr requirements.txt\n",
    "#%pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thwang99\\Documents\\disk\\2019-Masters-UCB\\2022-Summer\\w210-capstone\\project\\yolov5\n",
      "Setup complete. Using torch 1.11.0 (NVIDIA GeForce GTX 1080 Ti)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "%cd yolov5\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP6USLgz2f0r"
   },
   "source": [
    "# 2: Dataset information\n",
    "\n",
    "Below datasets can be found at https://universe.roboflow.com/thwang99\n",
    "\n",
    "DATASET1 (DD-Reduced-8-classes) contains the below classes and sample counts of each class (total 879, 8 classes):\n",
    "* c0 - Safe Driving: 82\n",
    "* c1 - Texting: 104\n",
    "* c2 - Talking on the phone: 137\n",
    "* c3 - Operating the Radio: 111\n",
    "* c4 - Drinking: 111\n",
    "* c5 - Reaching Behind: 112\n",
    "* c6 - Hair and Makeup: 112\n",
    "* c7 - Talking to Passenger: 100\n",
    "\n",
    "DATASET2 (DD-Reduced-10-classes) contains the below classes and sample counts of each class (total 879, 10 classes):\n",
    "* c0 - Safe Driving: 82\n",
    "* c1 - Texting - right: 60\n",
    "* c2 - Talking on the phone - right: 62\n",
    "* c3 - Operating the Radio: 111\n",
    "* c4 - Drinking: 111\n",
    "* c5 - Reaching Behind: 112\n",
    "* c6 - Hair and Makeup: 112\n",
    "* c7 - Talking to Passenger: 100\n",
    "* c8 - Texting - left: 54\n",
    "* c9 - Talking on the phone - left: 75\n",
    "\n",
    "The validation set contains 1381 samples for both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2jjT5uIHo6l5"
   },
   "outputs": [],
   "source": [
    "# set up environment. Replace with the path to your data files.\n",
    "# If downloading dataset from roboflow, download in YOLO5 format for pytorch.\n",
    "\n",
    "DATASET1_DIRECTORY = 'C:\\\\Users\\\\thwang99\\\\Downloads\\\\DD-Reduced2-8-classes.v1i.yolov5pytorch'\n",
    "DATASET2_DIRECTORY = 'C:\\\\Users\\\\thwang99\\\\Downloads\\\\DD-Reduced2-10-classes.v1i.yolov5pytorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7yAi9hd-T4B"
   },
   "source": [
    "# 3: Train and evaluate Our Custom YOLOv5 models\n",
    "\n",
    "Here, we are able to pass a number of arguments:\n",
    "- **img:** define input image size (we leave off since image sizes may differ)\n",
    "- **batch:** determine batch size\n",
    "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
    "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
    "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
    "- **cache:** cache images for faster training\n",
    "\n",
    "We use a lower number of epochs to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaFNnxLJbq4J",
    "outputId": "072d117a-12a6-4664-c13f-6bfd731a4c1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thwang99\\Downloads\\DD-Reduced2-8-classes.v1i.yolov5pytorch\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "#Train DATASET1 (8 classes)\n",
    "\n",
    "#You may need to manually modify the data.yaml files to refer to the dataset folders by full paths.\n",
    "\n",
    "#I also had to change, in dataloaders.py, to set num_workers to 0, as below:\n",
    "#    #nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers\n",
    "#    nw = 0\n",
    "\n",
    "dataset_dir = DATASET1_DIRECTORY + \"\\\\data.yaml\"\n",
    "print(dataset_dir)\n",
    "\n",
    "#You may want to run the below manually as it may ask for wandb.com login. It is also easier to see\n",
    "#progress from command line.\n",
    "#!python train.py --batch 16 --epochs 50 --data $dataset_dir --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "View and collect logs, and record results below. Results will be in \"runs\\train\\exp\", \"runs\\train\\exp1\", etc. Weights and biases results from a run:\n",
    "\n",
    "https://wandb.ai/thwang99/train/runs/10fleyi6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat run, note, this clears previous run's results!\n",
    "#shutil.rmtree('runs', ignore_errors=False, onerror=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thwang99\\Downloads\\DD-Reduced2-10-classes.v1i.yolov5pytorch\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "#Train DATASET1 (8 classes)\n",
    "\n",
    "#You may need to manually modify the data.yaml files to refer to the dataset folders by full paths.\n",
    "\n",
    "#I also had to change, in dataloaders.py, to set num_workers to 0, as below:\n",
    "#    #nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers\n",
    "#    nw = 0\n",
    "\n",
    "dataset_dir = DATASET2_DIRECTORY + \"\\\\data.yaml\"\n",
    "print(dataset_dir)\n",
    "\n",
    "#You may want to run the below manually as it may ask for wandb.com login. It is also easier to see\n",
    "#progress from command line.\n",
    "#!python train.py --batch 16 --epochs 50 --data $dataset_dir --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcIRLQOlA14A"
   },
   "source": [
    "# Results\n",
    "\n",
    "Weights and biases results from a run:\n",
    "\n",
    "https://wandb.ai/thwang99/train/runs/1n6rhlm7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison and summary\n",
    "\n",
    "The results are below, shown as two confusion matrix. Seperating or combining left/right both produce similar results. Combining left right results are slightly better actually. I suspect this maybe because when the left/right classes are seperated, there are fewer samples in each (approximately half the samples in each class).\n",
    "\n",
    "Dataset 1 (10 classes)\n",
    "![title](img/confusion_matrix-10-classes.png)\n",
    "\n",
    "Dataset 2 (8 classes)\n",
    "![title](img/confusion_matrix-8-classes.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Distracted-Driving-YOLOv5-Combine-Left-Right-Classes-Test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
